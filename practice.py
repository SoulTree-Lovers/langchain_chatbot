import streamlit as st
import time
from typing import List, Dict, Any
import tempfile
import os

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import ChatMessage
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

from dotenv import load_dotenv
from pdfminer.high_level import extract_text

load_dotenv()

# Custom CSS for modern UI
st.set_page_config(
    page_title="ğŸ¤– AI ê¸°ìˆ  ì „ë¬¸ê°€ ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide"
)

# Custom CSS
st.markdown("""
<style>
    .main {
        background-color: #f5f5f5;
    }
    .stChatMessage {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
        border: 1px solid #e0e0e0;
    }
    .stChatMessage[data-testid="stChatMessage"] {
        background-color: white;
    }
    .stButton>button {
        background-color: #4CAF50;
        color: white;
        border-radius: 20px;
        padding: 0.5rem 1rem;
    }
    .stTextInput>div>div>input {
        border-radius: 20px;
    }
    .css-1d391kg {
        padding: 1rem;
    }
    .document-list {
        margin: 1rem 0;
        padding: 1rem;
        background-color: white;
        border-radius: 0.5rem;
        border: 1px solid #e0e0e0;
    }
</style>
""", unsafe_allow_html=True)

# handle streaming conversation
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

def get_pdf_text(file):
    raw_text = extract_text(file)
    return raw_text

def process_uploaded_files(uploaded_files: List[Any]) -> Dict:
    if not uploaded_files:
        return None
    
    all_splits = []
    document_info = {}
    
    for uploaded_file in uploaded_files:
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name
        
        try:
            # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            raw_text = get_pdf_text(tmp_file_path)
            
            # í…ìŠ¤íŠ¸ ë¶„í• 
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=100,
                length_function=len,
                separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
            )
            splits = text_splitter.create_documents([raw_text])
            
            # ë¬¸ì„œ ì •ë³´ ì €ì¥
            document_info[uploaded_file.name] = {
                'splits': splits,
                'raw_text': raw_text
            }
            
            all_splits.extend(splits)
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp_file_path)
    
    if all_splits:
        # FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        vectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())
        return {
            'vectorstore': vectorstore,
            'document_info': document_info
        }
    return None

def generate_response(query_text: str, vectorstore, callback):
    # ë” ë§ì€ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    docs_list = vectorstore.similarity_search(query_text, k=5)
    docs = ""
    for i, doc in enumerate(docs_list):
        docs += f"ë¬¸ì„œ {i+1}:\n{doc.page_content}\n\n"
    
    # GPT-4 ëª¨ë¸ ì‚¬ìš©
    llm = ChatOpenAI(
        model_name="gpt-4",
        temperature=0.7,
        streaming=True,
        callbacks=[callback]
    )
    
    # ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜ ì„¤ì •
    rag_prompt = [
        SystemMessage(
            content="""ë‹¹ì‹ ì€ AI ê¸°ìˆ  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
            ì£¼ì–´ì§„ ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì „ë¬¸ì ì´ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
            ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ì¢…í•©ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
            ë¬¸ì„œì— ì •í™•í•œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°, 'ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
            ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”."""
        ),
        HumanMessage(
            content=f"ì§ˆë¬¸: {query_text}\n\nì°¸ê³  ë¬¸ì„œ:\n{docs}"
        ),
    ]

    response = llm(rag_prompt)
    return response.content

def generate_summarize(document_info: Dict, callback):
    llm = ChatOpenAI(
        model_name="gpt-4",
        temperature=0.7,
        streaming=True,
        callbacks=[callback]
    )
    
    summarize_prompt = PromptTemplate(
        input_variables=["text", "filename"],
        template="""ë‹¤ìŒ ë¬¸ì„œë¥¼ 3-4ê°œì˜ ì£¼ìš” í¬ì¸íŠ¸ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
        ë¬¸ì„œëª…: {filename}
        
        ë‚´ìš©:
        {text}
        
        ìš”ì•½ì€ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
        1. [ì£¼ìš” í¬ì¸íŠ¸ 1]
        2. [ì£¼ìš” í¬ì¸íŠ¸ 2]
        3. [ì£¼ìš” í¬ì¸íŠ¸ 3]
        """
    )
    
    chain = LLMChain(llm=llm, prompt=summarize_prompt)
    
    summaries = []
    for filename, info in document_info.items():
        response = chain.run(text=info['raw_text'], filename=filename)
        summaries.append(f"ğŸ“„ {filename}\n{response}\n")
    
    return "\n".join(summaries)

# UI êµ¬ì„±
st.title("ğŸ¤– AI ê¸°ìˆ  ì „ë¬¸ê°€ ì±—ë´‡")
st.markdown("""
ì´ ì±—ë´‡ì€ ì—¬ëŸ¬ AI ê¸°ìˆ  ë¬¸ì„œë¥¼ ë™ì‹œì— ë¶„ì„í•˜ê³  ì„¤ëª…í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
PDF ë¬¸ì„œë“¤ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•´ë³´ì„¸ìš”!
""")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“„ ë¬¸ì„œ ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader("PDF íŒŒì¼ë“¤ì„ ì„ íƒí•˜ì„¸ìš”", type=['pdf'], accept_multiple_files=True)
    
    if uploaded_files:
        with st.spinner("ë¬¸ì„œë“¤ì„ ì²˜ë¦¬ì¤‘ì…ë‹ˆë‹¤..."):
            result = process_uploaded_files(uploaded_files)
            if result:
                st.session_state['vectorstore'] = result['vectorstore']
                st.session_state['document_info'] = result['document_info']
                st.success("ëª¨ë“  ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
                
                # ì—…ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡ í‘œì‹œ
                st.markdown("### ğŸ“š ì—…ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡")
                for filename in result['document_info'].keys():
                    st.markdown(f"- {filename}")

# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(
            role="assistant",
            content="ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” AI ê¸°ìˆ  ì „ë¬¸ê°€ ì±—ë´‡ì…ë‹ˆë‹¤. PDF ë¬¸ì„œë“¤ì„ ì—…ë¡œë“œí•˜ê³  AI ê¸°ìˆ ì— ëŒ€í•´ ì–´ë–¤ ê²ƒì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"
        )
    ]

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg.role):
        st.write(msg.content)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append(ChatMessage(role="user", content=prompt))
    with st.chat_message("user"):
        st.write(prompt)

    with st.chat_message("assistant"):
        stream_handler = StreamHandler(st.empty())
        
        if prompt.lower() == "ìš”ì•½":
            if 'document_info' in st.session_state:
                response = generate_summarize(st.session_state['document_info'], stream_handler)
            else:
                response = "ë¬¸ì„œê°€ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
        else:
            if 'vectorstore' in st.session_state:
                response = generate_response(prompt, st.session_state['vectorstore'], stream_handler)
            else:
                response = "ë¬¸ì„œê°€ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        st.session_state["messages"].append(
            ChatMessage(role="assistant", content=response)
        ) 